{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### MNIST with Keras CNN + BN & DataAugment (99.X%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, utils\n",
    "from tensorflow.keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 1. Prepare train & test data (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_label), (test_data, test_label) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN에서는 [28행 x 28열]을 [1행 x 784열]로 펼쳐주지 않아도 됩니다.\n",
    "# 다만 이미지의 채널에 해당하는 차원을 마지막 차원으로 새로이 추가해줍니다. (MNIST는 흑백 이미지이므로 채널 수를 1로 지정합니다.)\n",
    "# 1 : channel[RGB]\n",
    "train_data = train_data.reshape(60000, 28, 28, 1) / 255.0 \n",
    "test_data = test_data.reshape(10000, 28, 28, 1) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.05923489, 0.11846978, 0.17770466, 0.23693955,\n",
       "        0.29617444, 0.35540933, 0.41464421, 0.4738791 , 0.53311399]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.normalize(np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = utils.to_categorical(train_label) # 0~9 -> one-hot vector\n",
    "test_label = utils.to_categorical(test_label) # 0~9 -> one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 2. Build the model & Set the criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
    "#                                     tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "#                                     tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(tf.keras.layers.Flatten())\n",
    "# model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "# model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 steps of Convolutional Neural Network\n",
    "# 1. Convolution (+ Batch Normalization)\n",
    "# 2. Activation\n",
    "# 3. (Max) Pooling\n",
    "# * Repeat 1~3 for adding more hidden layers.\n",
    "# 4. Connect the network to a fully-connected network (+ Dropout)\n",
    "# * This fully-connected network makes the model can result in classification.\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "\n",
    "# 텍스트 데이터, 시계열 데이터 등 사용 가능\n",
    "# Conv2D Transpose 역함수같은거\n",
    "# filters, kernel_size, strides, padding, Input shape : 이미지 한 장의 크기\n",
    "model.add(layers.Conv2D(32, (3, 3), input_shape=(28, 28, 1))) # Filter의 갯수, Filter의 shape, X data point의 shape (이미지 \"1장\"에 해당)\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "# Fully-connected network\n",
    "model.add(layers.Flatten()) # 전체 차원을 펼쳐줍니다.\n",
    "\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(rate=0.3))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 597,738\n",
      "Trainable params: 596,330\n",
      "Non-trainable params: 1,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: visualkeras==0.0.2 in c:\\users\\tech2_07\\anaconda3\\envs\\venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\tech2_07\\anaconda3\\envs\\venv\\lib\\site-packages (from visualkeras==0.0.2) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\tech2_07\\anaconda3\\envs\\venv\\lib\\site-packages (from visualkeras==0.0.2) (9.4.0)\n",
      "Requirement already satisfied: aggdraw>=1.3.11 in c:\\users\\tech2_07\\anaconda3\\envs\\venv\\lib\\site-packages (from visualkeras==0.0.2) (1.3.16)\n"
     ]
    }
   ],
   "source": [
    "!pip install visualkeras==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'visualkeras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-417c4e22bc66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mvisualkeras\u001b[0m \u001b[1;31m# https://github.com/paulgavrikov/visualkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvisualkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayered_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mImageFont\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruetype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arial.ttf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# font is optional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'visualkeras'"
     ]
    }
   ],
   "source": [
    "import visualkeras # https://github.com/paulgavrikov/visualkeras\n",
    "from PIL import ImageFont\n",
    "\n",
    "visualkeras.layered_view(model, legend=True, font=ImageFont.truetype(\"arial.ttf\", 13)) # font is optional\n",
    "\n",
    "# MacOS : \"arial.ttf\" -> \"Arial.ttf\" or \"AppleGothic\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(), \n",
    "              loss=losses.categorical_crossentropy, \n",
    "              metrics=[metrics.categorical_accuracy]) # Precision / Recall / F1-Score 적용하기 @ https://j.mp/3cf3lbi\n",
    "\n",
    "# model.compile(optimizer='adam', \n",
    "#               loss=losses.categorical_crossentropy, \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 3. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style='color:red; font-size:1.3em;'>ImageDataGenerator</span>** 를 활용하면 (Shift + Tab 참고)\n",
    "<br><br>\n",
    "\\- 다양한 **Data augmentation**을 적용할 수 있습니다.\n",
    "<br>\n",
    "\\- **rescale** 역시도 사전에 적용하지 않고 이 시점에 적용이 가능합니다. (ex. 1/255)\n",
    "<br>\n",
    "\\- **flow_from_directory()** 메서드를 사용하면 <span style='color:blue;'>**이미지 데이터 폴더로부터 direct하게 데이터를 가져와 training에 활용**</span>할 수도 있습니다.<br>-> 참고 : [ <span style='color:green;'>(Appendix) 2. Build, Train, and Visualize CNN models (CNN Basic)</span> ] > [ <span style='color:green;'>4. Use ImageDataGenerator for CNN models (for color-images).ipynb</span> ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "gen_train = ImageDataGenerator(rotation_range=8,        # Degree range for random rotations.\n",
    "                               shear_range=0.3,         # Shear Intensity (Shear angle in counter-clockwise direction in degrees)\n",
    "                               width_shift_range=0.08,  # Fraction of total width\n",
    "                               height_shift_range=0.08, # Fraction of total height\n",
    "                               zoom_range=0.08)         # Range for random zoom ([lower, upper] = [1-zoom_range, 1+zoom_range])\n",
    "\n",
    "train_generator = gen_train.flow(train_data, train_label, batch_size=64) # flow_from_directory() 활용 가능\n",
    "# train_generator = gen_train.flow_from_directory(~~~) # flow_from_directory() 활용 가능\n",
    "\n",
    "gen_test = ImageDataGenerator() # Test data에는 Augmentation을 적용하지 않습니다. (Test-time augmentation)\n",
    "\n",
    "test_generator = gen_test.flow(test_data, test_label, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "937"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Epoch 당 Batch 수 (batch_size == 64)\n",
    "\n",
    "60000 // 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937 [==============================] - 71s 75ms/step - loss: 0.1329 - categorical_accuracy: 0.9588 - val_loss: 0.0326 - val_categorical_accuracy: 0.9889\n",
      "Epoch 2/5\n",
      "937/937 [==============================] - 67s 72ms/step - loss: 0.0574 - categorical_accuracy: 0.9821 - val_loss: 0.0259 - val_categorical_accuracy: 0.9910\n",
      "Epoch 3/5\n",
      "937/937 [==============================] - 68s 72ms/step - loss: 0.0464 - categorical_accuracy: 0.9854 - val_loss: 0.0556 - val_categorical_accuracy: 0.9828\n",
      "Epoch 4/5\n",
      "937/937 [==============================] - 66s 70ms/step - loss: 0.0404 - categorical_accuracy: 0.9874 - val_loss: 0.0300 - val_categorical_accuracy: 0.9902\n",
      "Epoch 5/5\n",
      "937/937 [==============================] - 70s 75ms/step - loss: 0.0369 - categorical_accuracy: 0.9887 - val_loss: 0.0478 - val_categorical_accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# history = model.fit(train_data, train_label, batch_size=100, epochs=15, validation_data=(test_data, test_label))\n",
    "# 데이터 사이즈 // batch size\n",
    "history = model.fit(train_generator, \n",
    "                    steps_per_epoch=60000 // 64, # == number of batches\n",
    "                    epochs=5, \n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=10000 // 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0479 - categorical_accuracy: 0.9827\n",
      "loss (cross-entropy) : 0.04792989045381546\n",
      "test accuracy : 0.982699990272522\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(test_data, test_label, batch_size=100)\n",
    "\n",
    "print('loss (cross-entropy) :', result[0])\n",
    "print('test accuracy :', result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 예측 결과\n",
    "\n",
    "np.argmax(model.predict(test_data[:10]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 정답\n",
    "\n",
    "np.argmax(test_label[:10], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "#### 5. Visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = history.history['val_categorical_accuracy']\n",
    "acc = history.history['categorical_accuracy']\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_len = np.arange(len(acc))\n",
    "plt.plot(x_len, acc, marker='.', c='blue', label=\"Train-set Acc.\")\n",
    "plt.plot(x_len, val_acc, marker='.', c='red', label=\"Validation-set Acc.\")\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
