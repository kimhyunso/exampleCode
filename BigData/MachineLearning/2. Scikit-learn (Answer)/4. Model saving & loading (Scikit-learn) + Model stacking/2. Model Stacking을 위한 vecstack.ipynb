{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking을 위한 패키지 vecstack\n",
    "\n",
    "@ http://j.mp/34Etidr & http://j.mp/2NJQ9NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red;'>아래 명령어들로 xgboost & vecstack를 설치하고 Jupyter notebook 재부팅(Kernel Restart)을 진행해주세요</span>\n",
    "- 설치 중 에러가 발생하면 cmd(명령프롬프트) 우클릭 & 관리자권한으로 실행 후 명령어를 입력해 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost==1.5.2\n",
      "  Using cached xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from xgboost==1.5.2) (1.6.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from xgboost==1.5.2) (1.20.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vecstack==0.4.0\n",
      "  Using cached vecstack-0.4.0.tar.gz (18 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from vecstack==0.4.0) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from vecstack==0.4.0) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from vecstack==0.4.0) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->vecstack==0.4.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\tech2_07\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->vecstack==0.4.0) (1.0.1)\n",
      "Building wheels for collected packages: vecstack\n",
      "  Building wheel for vecstack (setup.py): started\n",
      "  Building wheel for vecstack (setup.py): finished with status 'done'\n",
      "  Created wheel for vecstack: filename=vecstack-0.4.0-py3-none-any.whl size=19880 sha256=83a03ce153657fa1a703c0f567536ff006d90b681ac0ca338204bdeba87a9755\n",
      "  Stored in directory: c:\\users\\tech2_07\\appdata\\local\\pip\\cache\\wheels\\17\\89\\0b\\21d5484cbf713c95b641ec1bdc40dd7ae798cbdea2337e3535\n",
      "Successfully built vecstack\n",
      "Installing collected packages: vecstack\n",
      "Successfully installed vecstack-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install vecstack==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Usage. Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "import warnings                 \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo data \n",
    "\n",
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test split \n",
    "# 8:2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1-st level models. \n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended. \n",
    "\n",
    "models = [ \n",
    "    ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), \n",
    "    RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), \n",
    "    XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [ExtraTreesClassifier]\n",
      "    fold  0:  [1.00000000]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.95000000] + [0.05000000]\n",
      "    FULL:     [0.95000000]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.96666667]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.94166667] + [0.04330127]\n",
      "    FULL:     [0.94166667]\n",
      "\n",
      "model  2:     [XGBClassifier]\n",
      "[10:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.93333333]\n",
      "[10:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.90000000]\n",
      "[10:32:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [1.00000000]\n",
      "[10:32:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.93333333] + [0.04082483]\n",
      "    FULL:     [0.93333333]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute stacking features\n",
    "\n",
    "# Training data x & y 에 대해 3가지 모델들을 모두 학습시킨 후,\n",
    "# 1) 해당 모델들이 X_train 에 대해 예측한 결과를 Data point 각각에 대해 묶어서 돌려준다. (S_train)\n",
    "# : Shape of X_train == (120, 4) -> Shape of S_train == (120, 3) <- 모델 각각의 예측치가 하나의 열\n",
    "# 2) 해당 모델들이 X_test 에 대해 예측한 결과를 Data point 각각에 대해 묶어서 돌려준다. (S_test)\n",
    "# : Shape of X_test == (30, 4) -> Shape of S_test == (30, 3) <- 모델 각각의 예측치가 하나의 열\n",
    "\n",
    "S_train, S_test = stacking(models, \n",
    "                           X_train, y_train, X_test, \n",
    "                           regression = False, \n",
    "                           metric = accuracy_score, \n",
    "                           n_folds = 4, stratified = True, shuffle = True, \n",
    "                           random_state = 0, verbose = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 2-nd level model \n",
    "\n",
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit 2-nd level model \n",
    "# 3개의 모델이 예측한 결과인 S_train을 Feature(3개의 열)로 하여 y_train을 맞추도록 모델을 학습시킨다.\n",
    "\n",
    "model = model.fit(S_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "# 앞서 3개의 모델이 예측한 결과인 S_test를 Feature로 하여 y_test를 예측한다.\n",
    "\n",
    "y_pred = model.predict(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: [0.96666667]\n"
     ]
    }
   ],
   "source": [
    "# Final prediction score\n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. Usage. Scikit-learn API (권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import StackingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your data\n",
    "\n",
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1st level estimators\n",
    "\n",
    "estimators = [ \n",
    "    ('ExtraTrees', ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)),\n",
    "    ('RandomForest', RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)),\n",
    "    ('XGB', XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StackingTransformer\n",
    "\n",
    "stack = StackingTransformer(estimators, \n",
    "                            regression = False, \n",
    "                            metric = accuracy_score, \n",
    "                            n_folds = 4, stratified = True, shuffle = True, \n",
    "                            random_state = 0, verbose = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "variant:      [A]\n",
      "n_estimators: [3]\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    fold  0:  [1.00000000]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.95000000] + [0.05000000]\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    fold  0:  [0.96666667]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.94166667] + [0.04330127]\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    fold  0:  [0.93333333]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.93333333] + [0.04082483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "# sc.fit(X_train) # X.train의 각 열에 대한 mean & std 계산\n",
    "# x_train_scaled = sc.transform(X_train)\n",
    "# y_train_scaled = sc.transform(Y_train)\n",
    "\n",
    "# Fit\n",
    "stack = stack.fit(X_train, y_train) # train x & y를 넣으면서 estimators에 있는 1st stage 모델들을 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get your stacked features\n",
    "\n",
    "S_train = stack.transform(X_train) # == predict\n",
    "S_test = stack.transform(X_test) # == predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: [0.96666667]\n"
     ]
    }
   ],
   "source": [
    "# Use 2nd level estimator with stacked features\n",
    "\n",
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') \n",
    "model = model.fit(S_train, y_train)\n",
    "\n",
    "y_pred = model.predict(S_test) \n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
