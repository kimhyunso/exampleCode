{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from matplotlib import rc\n",
    "import missingno as msno\n",
    "\n",
    "\n",
    "rc('font', family='Malgun Gothic') # Windows OS \n",
    "# rc('font', family='AppleGothic') # os x(mac os)  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\") # 그래프를 격자 스타일로 (숫자 범위가 눈에 잘 띄도록 ggplot 스타일 사용.)\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False # 그래프에서 마이너스 폰트 깨지는 문제 해결을 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>isAlone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age     Fare  Embarked  Title  FamilySize  isAlone\n",
       "0         0       3    0  22.0   7.2500         0      3           2        0\n",
       "1         1       1    1  38.0  71.2833         2      4           2        0\n",
       "2         1       3    1  26.0   7.9250         0      2           1        1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv(\"titanic.csv\")\n",
    "\n",
    "\n",
    "del titanic_df['Cabin'] # 너무 많은 결측치가 존재\n",
    "del titanic_df['PassengerId'] # Passenger 번호는 큰 의미를 갖고있지 않은 일련번호\n",
    "del titanic_df['Ticket'] # ticket 번호에서 패턴이 확인되지 않음\n",
    "\n",
    "\n",
    "titanic_df['Title'] = titanic_df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
    "\n",
    "rare_title = []\n",
    "for title in set(titanic_df['Title']):\n",
    "    if list(titanic_df['Title']).count(title) < 10:\n",
    "        rare_title.append(title)\n",
    "\n",
    "titanic_df['Title'] = titanic_df['Title'].replace('Mlle', 'Miss') # Mademoiselle\n",
    "titanic_df['Title'] = titanic_df['Title'].replace('Ms', 'Miss') \n",
    "titanic_df['Title'] = titanic_df['Title'].replace('Mme', 'Mrs') # Madame\n",
    "titanic_df['Title'] = titanic_df['Title'].replace(rare_title, 'Rare')\n",
    "\n",
    "del titanic_df['Name'] # 호칭만 남김\n",
    "\n",
    "title_mapping = {\"Master\":1, \"Miss\":2, \"Mr\":3, \"Mrs\":4, \"Rare\":5 }\n",
    "\n",
    "titanic_df['Title'] = titanic_df['Title'].map(title_mapping)\n",
    "titanic_df['Title'] = titanic_df['Title'].fillna(0)\n",
    "titanic_df['Title'].astype(int)\n",
    "\n",
    "\n",
    "sex_mapping = {\"male\": 0 , \"female\":1} \n",
    "titanic_df['Sex'] = titanic_df['Sex'].map(sex_mapping)\n",
    "\n",
    "\n",
    "titanic_df['Embarked'] = titanic_df['Embarked'].fillna('S')\n",
    "\n",
    "mapping_data ={\"S\":0, \"Q\":1, \"C\":2}\n",
    "titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].map(mapping_data)\n",
    "\n",
    "\n",
    "# titanic_df[\"Fareband\"] = pd.cut(titanic_df[\"Fare\"], 5) # Bin values into discrete intervals.\n",
    "# titanic_df[['Fareband','Survived']].groupby('Fareband').mean().sort_values(by='Survived', ascending=False)\n",
    "\n",
    "# del titanic_df['Fareband']\n",
    "\n",
    "# titanic_df.loc[ titanic_df['Fare'] <= 102, 'Fare'] = 0,\n",
    "# titanic_df.loc[(titanic_df['Fare'] > 102) & (titanic_df['Fare'] <= 204), 'Fare'] = 1,\n",
    "# titanic_df.loc[(titanic_df['Fare'] > 204) & (titanic_df['Fare'] <= 307), 'Fare'] = 2,\n",
    "# titanic_df.loc[ titanic_df['Fare'] > 307, 'Fare'] = 4\n",
    "\n",
    "\n",
    "titanic_df[\"FamilySize\"] = titanic_df[\"SibSp\"] + titanic_df[\"Parch\"] +1\n",
    "\n",
    "titanic_df['isAlone'] = 0\n",
    "titanic_df.loc[titanic_df['FamilySize'] == 1, 'isAlone'] = 1\n",
    "\n",
    "del titanic_df['SibSp']\n",
    "del titanic_df['Parch']\n",
    "\n",
    "# family_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\n",
    "# titanic_df['FamilySize'] = titanic_df['FamilySize'].map(family_mapping)\n",
    "\n",
    "\n",
    "titanic_df[\"Age\"].fillna(titanic_df.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n",
    "\n",
    "## 결측치를 별도의 머신러닝 모델을 만들어 채워넣을 수도 있음\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# titanic_df[\"Age\"].fillna(0, inplace=True)\n",
    "# f_columns = ['Pclass', 'Sex', 'Fare', 'Embarked', 'Title', 'FamilySize']\n",
    "# AgeNull = titanic_df[titanic_df['Age']==0]\n",
    "# AgeNotNull = titanic_df[titanic_df['Age']!=0]\n",
    "# X_train = AgeNotNull[f_columns]\n",
    "\n",
    "# random_forest = RandomForestRegressor(n_estimators=200)\n",
    "# random_forest.fit(X_train, AgeNotNull[\"Age\"]) # 나이 정보가 있는 열로 나이 열을 맞추기 위한 모델을 학습\n",
    "\n",
    "# train_pred = random_forest.predict(AgeNull[f_columns])\n",
    "# AgeNull[\"Age\"]= train_pred\n",
    "\n",
    "# titanic_df = AgeNotNull.append(AgeNull)\n",
    "\n",
    "\n",
    "# titanic_df['AgeBand'] = pd.cut(titanic_df['Age'], 5)\n",
    "# titanic_df[['AgeBand', 'Survived']].groupby('AgeBand', as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n",
    "\n",
    "# del titanic_df['AgeBand']\n",
    "\n",
    "# titanic_df.loc[ titanic_df['Age'] <= 16, 'Age'] = 0,\n",
    "# titanic_df.loc[(titanic_df['Age'] > 16) & (titanic_df['Age'] <= 32), 'Age'] = 1,\n",
    "# titanic_df.loc[(titanic_df['Age'] > 32) & (titanic_df['Age'] <= 48), 'Age'] = 2,\n",
    "# titanic_df.loc[(titanic_df['Age'] > 48) & (titanic_df['Age'] <= 64), 'Age'] = 3,\n",
    "# titanic_df.loc[ titanic_df['Age'] > 64, 'Age'] = 4\n",
    "\n",
    "titanic_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Age : 구간화 제외됨\n",
    "### - Fare : 구간화 제외됨\n",
    "### - FamilySize : Re-scaling 제외됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. X-Y Split & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_target = titanic_df['Survived'].copy()\n",
    "titanic_data = titanic_df.copy()\n",
    "del titanic_data['Survived']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(titanic_data, titanic_target, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 3. Make Pipeline for feature-transformer (StandardScaler & OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['Age', 'Fare']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'FamilySize', 'isAlone']\n",
    "categorical_transformer = OneHotEncoder(categories='auto') # categories='auto' : just for ignoring warning messages\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[ # List of (name, transformer, column(s))\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 4. Training with single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', GradientBoostingClassifier(n_estimators=200))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.8284\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(\"model score:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 5. Training with multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(solver='lbfgs'),\n",
    "          KNeighborsClassifier(n_neighbors=20),\n",
    "          DecisionTreeClassifier(),\n",
    "          ExtraTreeClassifier(),\n",
    "          AdaBoostClassifier(),\n",
    "          RandomForestClassifier(n_estimators=200),\n",
    "          GradientBoostingClassifier(n_estimators=200),\n",
    "          SVC(gamma='auto')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: LogisticRe ...\n",
      "model score: 0.8284\n",
      "\n",
      "model name: KNeighbors ...\n",
      "model score: 0.806\n",
      "\n",
      "model name: DecisionTr ...\n",
      "model score: 0.7873\n",
      "\n",
      "model name: ExtraTreeC ...\n",
      "model score: 0.7799\n",
      "\n",
      "model name: AdaBoostCl ...\n",
      "model score: 0.8209\n",
      "\n",
      "model name: RandomFore ...\n",
      "model score: 0.7985\n",
      "\n",
      "model name: GradientBo ...\n",
      "model score: 0.8284\n",
      "\n",
      "model name: SVC(gamma= ...\n",
      "model score: 0.8209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can make following codes as a function.\n",
    "# You can show the result knittly as a DataFrame.\n",
    "\n",
    "for model in models:\n",
    "    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                     ('classifier', model)])\n",
    "    \n",
    "    model_pipeline.fit(x_train, y_train)\n",
    "    accuracy = model_pipeline.score(x_test, y_test)\n",
    "    \n",
    "    print(\"model name:\", model.__str__()[:10], '...')\n",
    "    print(\"model score:\", round(accuracy, 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 6. Go deep with Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', GradientBoostingClassifier(n_estimators=200, random_state=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.8284\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train)\n",
    "\n",
    "accuracy = model.score(x_test, y_test)\n",
    "print(\"model score:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                        ('classifier', GradientBoostingClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'preprocessor', 'classifier', 'preprocessor__n_jobs', 'preprocessor__remainder', 'preprocessor__sparse_threshold', 'preprocessor__transformer_weights', 'preprocessor__transformers', 'preprocessor__verbose', 'preprocessor__num', 'preprocessor__cat', 'preprocessor__num__copy', 'preprocessor__num__with_mean', 'preprocessor__num__with_std', 'preprocessor__cat__categories', 'preprocessor__cat__drop', 'preprocessor__cat__dtype', 'preprocessor__cat__handle_unknown', 'preprocessor__cat__sparse', 'classifier__ccp_alpha', 'classifier__criterion', 'classifier__init', 'classifier__learning_rate', 'classifier__loss', 'classifier__max_depth', 'classifier__max_features', 'classifier__max_leaf_nodes', 'classifier__min_impurity_decrease', 'classifier__min_impurity_split', 'classifier__min_samples_leaf', 'classifier__min_samples_split', 'classifier__min_weight_fraction_leaf', 'classifier__n_estimators', 'classifier__n_iter_no_change', 'classifier__random_state', 'classifier__subsample', 'classifier__tol', 'classifier__validation_fraction', 'classifier__verbose', 'classifier__warm_start'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best params: {'classifier__learning_rate': 0.01, 'classifier__loss': 'exponential', 'classifier__max_depth': 4, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200, 'classifier__random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'classifier__loss': ['deviance', 'exponential'], # you can exclude \"deviance\" \n",
    "    'classifier__learning_rate': [0.01, 0.001], # you can exclude \"0.001\" \n",
    "    'classifier__n_estimators': [200, 500], # 500, 1000, 1500\n",
    "    'classifier__min_samples_split': [2, 4, 6],\n",
    "    'classifier__max_depth': [2, 4, 6],\n",
    "    'classifier__random_state': [0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, \n",
    "                           refit=True, cv=3, n_jobs=1, verbose=1, scoring= 'accuracy')\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2732710838317871"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.refit_time_ # Seconds used for refitting the best model on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.8284\n"
     ]
    }
   ],
   "source": [
    "accuracy = grid_search.score(x_test, y_test)\n",
    "print(\"model score:\", round(accuracy, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                         ('classifier', GradientBoostingClassifier(learning_rate=0.01, \n",
    "#                                                                   loss='exponential', \n",
    "#                                                                   max_depth=4, \n",
    "#                                                                   min_samples_split=2, \n",
    "#                                                                   n_estimators=200, \n",
    "#                                                                   random_state=0))])\n",
    "\n",
    "# model.fit(x_train, y_train)\n",
    "\n",
    "# accuracy = model.score(x_test, y_test)\n",
    "# print(\"model score:\", round(accuracy, 4))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
